{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1673f6d1-6a7a-418b-bff7-4362533a60c3",
   "metadata": {},
   "source": [
    "# In-Context Learning for Ophthalmology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabed6c-171d-4199-a636-90f6bdc46391",
   "metadata": {},
   "source": [
    "## Import libraries and define useful things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901ed6f-87d4-4a54-9a5c-13531c5315c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image as PIL_Image\n",
    "# import h5py\n",
    "# import typing\n",
    "\n",
    "import json\n",
    "\n",
    "import os, glob\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.special import softmax\n",
    "# from scipy.special import expit\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbc3b1-6ac1-4bc5-a2d8-f4ddb08429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dr_stages = [\"No Diabetic Retinopathy (Normal)\", \"Diabetic Retinopathy (DR)\"]\n",
    "dr_stages = [\"Normal\", \"Diabetic Retinopathy (DR)\"]\n",
    "onset_level = 1\n",
    "\n",
    "# IDRiD \n",
    "img_dir_tr = '../../EVAL_DATASETS/IDRiD/grading/OriginalImages/training/'\n",
    "# full_path_list_tr = sorted(glob.glob(img_dir_tr + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_tr}\\t{len(full_path_list_tr)}', flush=True)\n",
    "\n",
    "csv_file_tr = '../../EVAL_DATASETS/IDRiD/grading/Groundtruths/IDRiD_TrainingLabels.csv'\n",
    "df_metadata_tr = pd.read_csv(csv_file_tr, low_memory=False)\n",
    "df_metadata_tr = df_metadata_tr[['Image name', 'Retinopathy grade', 'Risk of macular edema ']]\n",
    "label_text = []\n",
    "file_paths = []\n",
    "file_paths_224 = []\n",
    "split = []\n",
    "for idx, row in df_metadata_tr.iterrows():\n",
    "    if int(row['Retinopathy grade']) < onset_level: \n",
    "        label_bin = 0\n",
    "    else:\n",
    "        label_bin = 1\n",
    "    label_text.append(dr_stages[label_bin])\n",
    "    file_paths.append(img_dir_tr + str(row['Image name']) + '.jpg')\n",
    "    file_paths_224.append(img_dir_tr + 'idrid_224/' + str(row['Image name']) + '.png')\n",
    "    split.append('train')\n",
    "df_metadata_tr['label_text'] = label_text\n",
    "df_metadata_tr['file_path'] = file_paths\n",
    "df_metadata_tr['file_path_224'] = file_paths_224\n",
    "df_metadata_tr['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_tr.shape}')\n",
    "print(df_metadata_tr.columns)\n",
    "\n",
    "img_dir_te = '../../EVAL_DATASETS/IDRiD/grading/OriginalImages/test/'\n",
    "# full_path_list_te = sorted(glob.glob(img_dir_te + '*' + '.jpg', recursive=False))\n",
    "# print(f'Number of files in {img_dir_te}\\t{len(full_path_list_te)}', flush=True)\n",
    "\n",
    "csv_file_te = '../../EVAL_DATASETS/IDRiD/grading/Groundtruths/IDRiD_TestLabels.csv'\n",
    "df_metadata_te = pd.read_csv(csv_file_te, low_memory=False)\n",
    "label_text = []\n",
    "file_paths = []\n",
    "file_paths_224 = []\n",
    "split = []\n",
    "for idx, row in df_metadata_te.iterrows():\n",
    "    if int(row['Retinopathy grade']) < onset_level: \n",
    "        label_bin = 0\n",
    "    else:\n",
    "        label_bin = 1\n",
    "    label_text.append(dr_stages[label_bin])\n",
    "    file_paths.append(img_dir_te + str(row['Image name']) + '.jpg')\n",
    "    file_paths_224.append(img_dir_te + 'idrid_224/' + str(row['Image name']) + '.png')\n",
    "    split.append('test')\n",
    "df_metadata_te['label_text'] = label_text\n",
    "df_metadata_te['file_path'] = file_paths\n",
    "df_metadata_te['file_path_224'] = file_paths_224\n",
    "df_metadata_te['split'] = split\n",
    "print(f'Metadata shape : {df_metadata_te.shape}')\n",
    "print(df_metadata_te.columns)\n",
    "\n",
    "df_metadata = pd.concat([df_metadata_tr, df_metadata_te], axis=0)\n",
    "print(f'Metadata shape : {df_metadata.shape}')\n",
    "print(df_metadata.columns)\n",
    "\n",
    "del df_metadata_tr, df_metadata_te, file_paths, file_paths_224, label_text, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7726c0-ebd3-4edf-aec0-28f8cf3c9b1a",
   "metadata": {},
   "source": [
    "## Preprare RETFound and extract feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044a415-1f83-495a-9702-b278f4d48e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '../RETFound_MAE/')\n",
    "\n",
    "import torch\n",
    "import models_vit\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import transforms as T\n",
    "# from torchvision.transforms import v2 as T\n",
    "\n",
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(f'Max float : {sys.float_info.max}')\n",
    "print(torch.__version__)\n",
    "print(f'Cuda available : {torch.cuda.is_available()}')\n",
    "print(f'Number of GPUs : {torch.cuda.device_count()}')\n",
    "print(f'CUDA Version : {torch.version.cuda}')\n",
    "print(f'timm Version : {timm.__version__}')\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='vit_large_patch16'):\n",
    "    # build model\n",
    "    model = models_vit.__dict__[arch](\n",
    "        img_size=224,\n",
    "        num_classes=5,\n",
    "        drop_path_rate=0,\n",
    "        global_pool=True,\n",
    "    )\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Device : {device}')\n",
    "\n",
    "chkpt_dir = '../../Projects/RETFound_MAE/RETFound_cfp_weights.pth'\n",
    "vision_encoder = prepare_model(chkpt_dir, 'vit_large_patch16')\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "vision_encoder.to(device)\n",
    "print('Vision encoder model loaded.')\n",
    "\n",
    "transforms = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD), \n",
    "])\n",
    "\n",
    "\n",
    "class IDRiD_ImageDataset(Dataset):\n",
    "    def __init__(self, metadata, target_column='Retinopathy grade', \n",
    "                 transforms=None, target_transforms=None\n",
    "                ):\n",
    "        self.metadata = metadata \n",
    "        self.target_column = target_column        \n",
    "        self.transforms = transforms\n",
    "        self.target_transforms = target_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        filepath = self.metadata.iloc[idx]['file_path_224']\n",
    "        with PIL_Image.open(filepath) as img:\n",
    "\n",
    "            if len(img.size) < 3: # if single channel, convert to RGB\n",
    "                img = img.convert(mode='RGB')\n",
    "            \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "\n",
    "        if int(self.metadata.iloc[idx][self.target_column]) < onset_level: \n",
    "            label_bin = 0\n",
    "        else:\n",
    "            label_bin = 1\n",
    "        \n",
    "        return img, label_bin #, os.path.basename(filepath), self.metadata.iloc[idx]['split']\n",
    "    \n",
    "    # def get_labels(self):\n",
    "    #     # return as series for ImbalancedDatasetSampler to read into a Pandas dataframe\n",
    "    #     return self.metadata[self.target_column]\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "num_workers = 8\n",
    "batch_size = 32 # 32 // n_views #52 # 128 # 96 # 128 # 208 # 164 # 112 # 75 # 22*4\n",
    "\n",
    "# Note that shuffle is mutually exclusive with Sampler\n",
    "# shuffle_dict = {'train': False, 'test': False} #, 'test': False}\n",
    "\n",
    "idrid_dataset = IDRiD_ImageDataset(df_metadata, transforms=transforms, target_transforms=None)\n",
    "\n",
    "dataloader = DataLoader(idrid_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, sampler=None, # samplers[split], \n",
    "                        num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19e27a-f98e-46de-b290-92c261bca511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(vision_encoder, dataloader):\n",
    "    \n",
    "    out_data = OrderedDict()\n",
    "    out_data['features'] = []\n",
    "    out_data['labels'] = []\n",
    "    \n",
    "    vision_encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(iter(dataloader)):\n",
    "                    \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # a dictionary of features from various read-out layers\n",
    "            # {readout_layer_name : features}\n",
    "            # with torch.autocast(device_type='cuda', dtype=torch.float16): #torch.cuda.amp.autocast():\n",
    "                # with torch.inference_mode(mode=True):\n",
    "            # outputs = model(inputs)\n",
    "            outputs = vision_encoder.forward_features(inputs)\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            # for readout_layername, features in outputs.items():\n",
    "            outputs = np.squeeze(outputs.cpu().detach().numpy())\n",
    "            out_data['features'].append(outputs)\n",
    "            out_data['labels'].append(labels)\n",
    "            # break # only 1 readout layer name!!\n",
    "    \n",
    "    \n",
    "    # list to numpy array\n",
    "    out_data['features'] = np.concatenate(out_data['features'], axis=0) \n",
    "    out_data['labels'] = np.concatenate(out_data['labels'], axis=0) \n",
    "        \n",
    "    print(f'Features : {out_data[\"features\"].shape}') \n",
    "    print(f'Labels : {out_data[\"labels\"].shape}, Unique labels : {np.unique(out_data[\"labels\"], return_counts=True)}') \n",
    "    \n",
    "    return out_data\n",
    "\n",
    "out_data = extract_features(vision_encoder, dataloader)\n",
    "\n",
    "X, y = out_data['features'], np.asarray(out_data['labels'], dtype=np.int32)\n",
    "    \n",
    "with open(f'IDRiD_Binary_Features.npy', 'wb') as handle:\n",
    "    # pickle.dump(out_data, handle, protocol=4)\n",
    "    np.save(handle, out_data['features'])\n",
    "    np.save(handle, out_data['labels'])\n",
    "\n",
    "del out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a856c99-6f30-4be4-b819-7ce4d434b802",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{np.unique(y, return_counts=True)[1]/np.sum(np.unique(y, return_counts=True)[1])}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
